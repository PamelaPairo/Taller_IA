---
output:
   xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    seal: false
    self_contained: true
    nature:
      highlightStyle: github
      highlightLines: false
      countIncrementalSlides: false
---

class: title-slide, center, middle
background-image: url(images/portada.jpg)
background-size: cover

#Clase 2: Aprendizaje Supervisado
### Pamela E. Pairo

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)
library(knitr)
library(tidyverse)
# set default options
opts_chunk$set(echo=FALSE,
               collapse = TRUE,
               fig.width = 7.252,
               fig.height = 4,
               dpi = 300)
xaringanExtra::use_tile_view()
xaringanExtra::use_clipboard()
xaringanExtra::use_webcam(width = 210, height = 220)
xaringanExtra::use_share_again()
xaringanExtra::use_tachyons()
xaringanExtra::style_share_again(
  share_buttons = c("twitter", "linkedin", "pocket")
)
xaringanExtra::use_panelset()
```

```{r xaringan-extra-styles, include=FALSE}
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         #<<
  mute_unhighlighted_code = TRUE  #<<
)
```


```{r xaringan-logo, echo=FALSE}

xaringanExtra::use_fit_screen()
xaringanExtra::use_logo(
  image_url = "images/uade.jpg"
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("PT Sans", "300", "300i"),
  code_font_google   = google_font("Fira Mono")

)

colors = c(
  red = "#f34213",
  purple = "#3e2f5b",
  orange = "#ff8811",
  green = "#136f63",
  white = "#FFFFFF"
)
```

---
# En la clase de hoy...

--

### En la primera parte:

- Regresi칩n Log칤stica 
- Na칦ve Bayes
- Support Vector Machine (SVM)
- 츼rboles de decisi칩n
- Random Forest 

 

--

### **.orange[Recreo]** `r emo::ji("coffee")`游븰

--

### En la segunda parte:

- M칠tricas

---

class: inverse, middle, center

#Regresi칩n Log칤stica

---

#Regresi칩n Log칤stica

Utiliza un **.purple[enfoque probabil칤stico]**

.center[
<img src="images/logistic.png" width="50%"/>
]


.center[
<img src="images/sigmoidea.png" width="45%"/>
]

---
# Regresi칩n Log칤stica

Se define un umbral (en este caso 0.5):

.center[

Se predice  $y= 1$ si $h_{\theta}(x) \ge 0.5$

Se predice  $y= 0$ si $h_{\theta}(x) < 0.5$


<img src="images/umbral.png" width="45%"/>
]

.footnote[Extra칤do del material did치ctico de Cristian Cardellino]
---

## Regresi칩n Log칤stica: Funci칩n de Costo

.center[
<img src="images/costo_log.png" width="85%"/>

<img src="images/cost_log_plot.png" width="85%"/>
]

---
class: inverse, middle, center

# Naive Bayes

---

class: inverse, middle, center

# Support Vector Machine (SVM)

---
# SVM

.pull-left[<img src="images/example_svm.png" width="100%"/>]

.pull-right[<img src="images/example_svm.png" width="100%"/>]

---
# SVM

.bg-washed-light-purple.b--light-purple.ba.bw2.br2.shadow-5.ph2.mt2[

El algoritmo de **SVM** encuentra el hiperplano que devuelve el **mayor margen** entre s칤 mismo y los vectores de soporte

]
---
# SVM

.center[<img src="images/svm.png" width="85%"/>]

.footnote[Imagen extra칤da de [este link](https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm)]
---

class: inverse, middle, center

# 츼rboles de decisi칩n

---

## 츼rboles de decisi칩n

Funcionan bien para datos no linealmente separables.

Se quiere predecir 3 especies de _Iris_ a partir del ancho y largo del s칠palo: _Iris setosa_, _Iris versicolor_ y _Iris virginica_.

--

.pull-left[
<img src="images/iris.png" width="100%"/>
]

--

.pull-right[
<img src="images/iris_plot.png" width="100%"/>
]

---

## Hiperpar치metro

.bg-washed-light-purple.b--light-purple.ba.bw2.br3.shadow-5.ph4.mt5[
Valores no aprendidos por el algoritmo desde los datos y por ende deben ser seteados antes de entrenar el algoritmo.

]

--

### En 치rboles de decisi칩n:

- `n_min` : n m칤nimo para dividir los nodos

- `tree_depth`: l칤mite a la profundidad del 치rbol

- `cost_complexity`: costo o penalizaci칩n a los errores de 치rboles m치s complejos. Es una forma de poda.

---

# Ensamble learning: Bagging

Los **.orange[치rboles de decisi칩n]** son algoritmos inestables debido a que peque침as variaciones en el dataset pueden generar modelos muy diferentes.
--

.bg-washed-light-purple.b--light-purple.ba.bw2.br3.shadow-5.ph4.mt4[
**.orange[Bagging (Bootstrap Aggregation)]** es un m칠todo para hacer aprendizaje por _ensemble_.

Consiste en realizar K subsets del dataset aleatoriamente y con reemplazo, resultando en un _ensamble_ de K modelos. La asignaci칩n de la clase se realiza por mayoria simple en casos de clasificaci칩n.

]
---
class: inverse, middle, center

#`r emo::ji("computer")`
###Demo 츼rboles de decisi칩n

---
class: inverse, middle, center

# Random Forest
---
# Random Forest

Son una modificaci칩n a Bagging para 츼rboles de Decisi칩n. En cada 치rbol se consideran s칩lo M atributos elegidos aleatoriamente.

El algoritmo es sencillo, f치cil de implementar, f치cil de usar y requiere de poco ajuste de par치metros.

.center[
<img src="images/random_forest.png" width="55%"/>
]
---
class: inverse, middle, center

## Descanso

```{r}

library(countdown)
countdown(minutes = 15, seconds = 0, font_size="7em", color_background = "white")

```

---
# Referencias

- [Introduction to Logistic Regression](https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148), art칤culo publicado en Towards to Data Science
